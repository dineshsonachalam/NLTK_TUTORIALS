{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of Speech tagging - means labeling words in a sentence as nouns, adjectives, verbs...etc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent's\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PunktSentenceTokenizer\n",
    "- This tokenizer is capable of unsupervised machine learning, so you can actually train it on any body of text that you use.\n",
    "- PunktSentenceTokenizer is the abstract class for the default sentence tokenizer, i.e. sent_tokenize(), provided in NLTK.\n",
    "- PunktSentenceTokenizer is an sentence boundary detection algorithm that must be trained to be used\n",
    " \n",
    " NLTK already includes a pre-trained version of the PunktSentenceTokenizer.<br>\n",
    " So if you use initialize the tokenizer without any arguments, it will default to the pre-trained version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My friend holds a Msc.', 'in Computer Science.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So if you use initialize the tokenizer without any arguments, it will default to the pre-trained version:\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "txt = \"My friend holds a Msc. in Computer Science.\"\n",
    "tokenizer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dinesh is a Msc.', 'holder in Artificial Intelligence.', 'He is super awesome.']\n"
     ]
    }
   ],
   "source": [
    "# You can also provide your own training data to train the tokenizer before using it. \n",
    "# Punkt tokenizer uses an unsupervised algorithm, meaning you just train it with regular text.\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "# I am going to train this text to correct tokenizing this sentence\n",
    "train_text = \"My friend holds a Msc. in Computer Science.He is super awesome.\" \n",
    "\n",
    "# Train_txt will be learning from this text\n",
    "learn_text = \"Dinesh is a Msc. holder in Artificial Intelligence. \"\n",
    "\n",
    "# we can train the Punkt tokenizer like:\\\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "# <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x0000029815CAD208>\n",
    "#print(custom_sent_tokenizer)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(learn_text)\n",
    "print(tokenized)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
