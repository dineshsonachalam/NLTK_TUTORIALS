{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of Speech tagging - means labeling words in a sentence as nouns, adjectives, verbs...etc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent's\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PunktSentenceTokenizer\n",
    "- This tokenizer is capable of unsupervised machine learning, so you can actually train it on any body of text that you use.\n",
    "- PunktSentenceTokenizer is the abstract class for the default sentence tokenizer, i.e. sent_tokenize(), provided in NLTK.\n",
    "- PunktSentenceTokenizer is an sentence boundary detection algorithm that must be trained to be used\n",
    "- The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
    " \n",
    " NLTK already includes a pre-trained version of the PunktSentenceTokenizer.<br>\n",
    " So if you use initialize the tokenizer without any arguments, it will default to the pre-trained version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My friend holds a Msc.', 'in Computer Science.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So if you use initialize the tokenizer without any arguments, it will default to the pre-trained version:\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "txt = \"My friend holds a Msc. in Computer Science.\"\n",
    "tokenizer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 .TRAIN_TEXT: PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "February 2, 2005\n",
      "\n",
      "\n",
      "9:10 P.M. EST \n",
      "\n",
      "THE PRESIDENT: Mr. Speaker, Vice President Cheney, members of Congress, fellow citizens: \n",
      "\n",
      "As a new Congress gathers, all of us in the elected branches of government share a great privilege: We've been placed in office by the votes of the people we serve. \n",
      "\n",
      "1 .SAMPLE_TEXT: \t PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "January 31, 2006\n",
      "\n",
      "THE PRESIDENT: Thank you all. \n",
      "\n",
      "1 .LEARNT_TEXT: \t PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "January 31, 2006\n",
      "\n",
      "THE PRESIDENT: Thank you all. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2 .TRAIN_TEXT: And tonight that is a privilege we share with newly-elected leaders of Afghanistan, the Palestinian Territories, Ukraine, and a free and sovereign Iraq. \n",
      "\n",
      "2 .SAMPLE_TEXT: \t Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream. \n",
      "\n",
      "2 .LEARNT_TEXT: \t Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3 .TRAIN_TEXT: (Applause.) \n",
      "\n",
      "3 .SAMPLE_TEXT: \t Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King. \n",
      "\n",
      "3 .LEARNT_TEXT: \t Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4 .TRAIN_TEXT: Two weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all. \n",
      "\n",
      "4 .SAMPLE_TEXT: \t (Applause.) \n",
      "\n",
      "4 .LEARNT_TEXT: \t (Applause.) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 .TRAIN_TEXT: This evening I will set forth policies to advance that ideal at home and around the world. \n",
      "\n",
      "5 .SAMPLE_TEXT: \t President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan. 31, 2006. \n",
      "\n",
      "5 .LEARNT_TEXT: \t President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan. 31, 2006. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Actual Text\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "# we can train the Punkt tokenizer like:(Creates a PunktSentenceTokenizer Object \n",
    "# <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x0000029815D0B860>)\n",
    "\n",
    "custom_sent_tokenizer_1 = PunktSentenceTokenizer(sample_text)\n",
    "custom_sent_tokenizer_2 = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "# Training the Punkt Tokenizer:\n",
    "# Punkt tokenizer uses an unsupervised algorithm, meaning you just train it with regular text. \n",
    "# custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "custom_sent_tokenizer_3 = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "\n",
    "# Then we can actually tokenize, using: PunktSenteceTokenizer_Object.Tokenize_method() ->Tokenizing by sentence \n",
    "# Since its a sentence tokenizer\n",
    "tokenized_1 = custom_sent_tokenizer_1.tokenize(sample_text)\n",
    "tokenized_2 = custom_sent_tokenizer_2.tokenize(train_text)  \n",
    "\n",
    "\n",
    "\n",
    "tokenized_3 = custom_sent_tokenizer_3.tokenize(sample_text) # contains 2006 data\n",
    "\n",
    "line_count = 1\n",
    "\n",
    "for i in tokenized_1[0:5]: # We are Limiting to first 10 sentence\n",
    "   \n",
    "    print(line_count,\".TRAIN_TEXT:\",tokenized_2[line_count-1],\"\\n\") # contains 2005 data\n",
    "    \n",
    "    print(line_count,\".SAMPLE_TEXT:\",\"\\t\",tokenized_1[line_count-1],\"\\n\") # contains 2006 data\n",
    "    \n",
    "    print(line_count,\".LEARNT_TEXT:\",\"\\t\",tokenized_3[line_count-1],\"\\n\\n\\n\\n\\n\") # contains 2006 data\n",
    "    line_count = line_count+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "February 2, 2005\n",
      "\n",
      "\n",
      "9:10 P.M. EST \n",
      "\n",
      "THE PRESIDENT: Mr. Speaker, Vice President Cheney, members of Congress, fellow citizens: \n",
      "\n",
      "As a new Congress gathers, all of us in the elected branches of government share a great privilege: We've been placed in office by the votes of the people we serve. \n",
      "\n",
      "2 \t And tonight that is a privilege we share with newly-elected leaders of Afghanistan, the Palestinian Territories, Ukraine, and a free and sovereign Iraq. \n",
      "\n",
      "3 \t (Applause.) \n",
      "\n",
      "4 \t Two weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all. \n",
      "\n",
      "5 \t This evening I will set forth policies to advance that ideal at home and around the world. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Actual Text\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "# we can train the Punkt tokenizer like:(Creates a PunktSentenceTokenizer Object \n",
    "# <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x0000029815D0B860>)\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(sample_text)\n",
    "\n",
    "# Then we can actually tokenize, using: PunktSenteceTokenizer_Object.Tokenize_method() ->Tokenizing by sentence \n",
    "# Since its a sentence tokenizer\n",
    "tokenized = custom_sent_tokenizer.tokenize(train_text) \n",
    "#Tokenized is a list storing the sentence tokenized text  -sentence by sentence \n",
    "\n",
    "# Printing the tokenized list Element line by line\n",
    "\n",
    "# Error:\n",
    "line_count = 1\n",
    "for i in tokenized[0:5]: # We are Limiting to first 5 sentence\n",
    "    print(line_count,\"\\t\",i,\"\\n\")\n",
    "    line_count = line_count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "January 31, 2006\n",
      "\n",
      "THE PRESIDENT: Thank you all. \n",
      "\n",
      "2 \t Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream. \n",
      "\n",
      "3 \t Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King. \n",
      "\n",
      "4 \t (Applause.) \n",
      "\n",
      "5 \t President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan. 31, 2006. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Actual Text\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "# we can train the Punkt tokenizer like:(Creates a PunktSentenceTokenizer Object \n",
    "# <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x0000029815D0B860>)\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "# Then we can actually tokenize, using: PunktSenteceTokenizer_Object.Tokenize_method() ->Tokenizing by sentence \n",
    "# Since its a sentence tokenizer\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text) \n",
    "#Tokenized is a list storing the sentence tokenized text  -sentence by sentence \n",
    "\n",
    "# Printing the tokenized list Element line by line\n",
    "\n",
    "# Error:\n",
    "line_count = 1\n",
    "for i in tokenized[0:5]: # We are Limiting to first 5 sentence\n",
    "    print(line_count,\"\\t\",i,\"\\n\")\n",
    "    line_count = line_count+1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
